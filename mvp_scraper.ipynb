{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c67eb34-34ea-4588-9186-6fa6b8524ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85db143c-2042-40ea-b881-2459de793a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraped for 1983\n",
      "Data scraped for 1984\n",
      "Data scraped for 1985\n",
      "Data scraped for 1986\n",
      "Data scraped for 1987\n",
      "Data scraped for 1988\n",
      "Data scraped for 1989\n",
      "Data scraped for 1990\n",
      "Data scraped for 1991\n",
      "Data scraped for 1992\n",
      "Data scraped for 1993\n",
      "Data scraped for 1994\n",
      "Data scraped for 1995\n",
      "Data scraped for 1996\n",
      "Data scraped for 1997\n",
      "Data scraped for 1998\n",
      "Data scraped for 1999\n",
      "Data scraped for 2000\n",
      "Data scraped for 2001\n",
      "Data scraped for 2002\n",
      "Data scraped for 2003\n",
      "Data scraped for 2004\n",
      "Data scraped for 2005\n",
      "Data scraped for 2006\n",
      "Data scraped for 2007\n",
      "Data scraped for 2008\n",
      "Data scraped for 2009\n",
      "Data scraped for 2010\n",
      "Data scraped for 2011\n",
      "Data scraped for 2012\n",
      "Data scraped for 2013\n",
      "Data scraped for 2014\n",
      "Data scraped for 2015\n",
      "Data scraped for 2016\n",
      "Data scraped for 2017\n",
      "Data scraped for 2018\n",
      "Data scraped for 2019\n",
      "Data scraped for 2020\n",
      "Data scraped for 2021\n",
      "Data scraped for 2022\n",
      "Data scraped for 2023\n"
     ]
    }
   ],
   "source": [
    "def scrape_baseball_reference(url, writer):\n",
    "    # Fetch the HTML content of the page\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch page\")\n",
    "        return\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find AL MVP table\n",
    "    al_mvp_table = soup.find('table', {'id': 'AL_MVP_voting'})\n",
    "    if al_mvp_table:\n",
    "        scrape_table(al_mvp_table, writer, url)\n",
    "\n",
    "    # Find NL MVP table\n",
    "    nl_mvp_table = soup.find('table', {'id': 'NL_MVP_voting'})\n",
    "    if nl_mvp_table:\n",
    "        scrape_table(nl_mvp_table, writer, url)\n",
    "\n",
    "def scrape_table(table, writer, year_url):\n",
    "    year = year_url.split('_')[-1].split('.')[0]  # Extract year from URL\n",
    "    # Extract table rows\n",
    "    for tr in table.find_all('tr')[1:]:\n",
    "        row_data = [year]  # Add year as the first element\n",
    "        row_data.extend(td.text.strip() for td in tr.find_all('td'))\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "def gen_base_urls(start_yr, end_yr):\n",
    "    base_urls = []\n",
    "    for year in range(start_yr, end_yr + 1):\n",
    "        base_url = f\"https://www.baseball-reference.com/awards/awards_{year}.shtml\"\n",
    "        base_urls.append(base_url)\n",
    "    return base_urls\n",
    "    \n",
    "# Output CSV file\n",
    "output_file = 'baseball_awards.csv'\n",
    "\n",
    "# Open CSV file for writing with UTF-8 encoding\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write header row\n",
    "    column_headers = [\"Year\", \"Name\", \"Tm\", \"Vote Pts\", \"1st Place\", \"Share\", \"WAR\", \"G\", \"AB\", \"R\", \"H\", \"HR\", \"RBI\", \"SB\", \"BB\", \"BA\", \"OBP\", \"SLG\", \"OPS\", \"W\", \"L\", \"ERA\", \"WHIP\", \"G\", \"GS\", \"SV\", \"IP\", \"H\", \"HR\", \"BB\", \"SO\"]\n",
    "    writer.writerow(column_headers)\n",
    "\n",
    "    base_urls = gen_base_urls(1983,2023)\n",
    "    \n",
    "    # Loop through the base URLs\n",
    "    for base_url in base_urls:\n",
    "        # Scrape data for the current year\n",
    "        scrape_baseball_reference(base_url, writer)\n",
    "        \n",
    "        year = base_url.split('_')[-1].split('.')[0]\n",
    "        print(f\"Data scraped for {year}\")\n",
    "        \n",
    "        # Sleep for a short while to avoid overwhelming the server\n",
    "        time.sleep(3)  # Adjust as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
